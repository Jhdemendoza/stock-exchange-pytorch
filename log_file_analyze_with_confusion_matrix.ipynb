{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_folder = 'prev_logs/'\n",
    "log_folder = 'logs/random_tickers/'\n",
    "\n",
    "def agg_by_threshold(thresholds):\n",
    "    files = sorted(os.listdir(log_folder))\n",
    "    by_threshold = []\n",
    "    for threshold in thresholds:\n",
    "        by_threshold += [[file for file in files if threshold in file.split('_')[-1]]]\n",
    "    return by_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key(original_line):\n",
    "    result = []\n",
    "    of_interest = ['block_depth', \n",
    "                   'const_factor', \n",
    "                   'learning_rate', \n",
    "                   'linear_dim', \n",
    "                   'percentile',\n",
    "                  ]\n",
    "    for item in of_interest:\n",
    "        pattern = re.compile(item+'=[\\d.]*')\n",
    "        result += [re.search(pattern, original_line).group(0)]\n",
    "    return ','.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_log_file(file_path):\n",
    "    split_colon = lambda x: x.split(':')\n",
    "\n",
    "    test_data = (pd.read_csv(file_path)\n",
    "                   .iloc[:-1, 2:]\n",
    "                   .iloc[1::2]\n",
    "                   .iloc[:, 1:9]\n",
    "                   .applymap(split_colon))\n",
    "    \n",
    "    if test_data.empty:\n",
    "        return None\n",
    "    \n",
    "    names = (test_data.applymap(lambda x: x[0])\n",
    "                      .iloc[0]\n",
    "                      .values\n",
    "                      .tolist())\n",
    "    names = list(map(lambda x: x.replace(' ', ''), names))\n",
    "    dataframe = (test_data.iloc[:, :-1].applymap(lambda x: float(x[-1]))\n",
    "                          .copy())\n",
    "    confusion_matrix_ = test_data.iloc[:, -1].map(lambda x: x[-1]).copy()\n",
    "    dataframe = pd.concat((dataframe, confusion_matrix_), axis=1)\n",
    "    dataframe.columns = names\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_and_df(file_name):\n",
    "    file_path = log_folder + file_name\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        line = f.readlines()[0]\n",
    "        key = create_key(line)\n",
    "    \n",
    "    df = process_single_log_file(file_path)\n",
    "    \n",
    "    if df is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        return key, df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_key_df(file_name):\n",
    "    key, df = get_key_and_df(file_name)\n",
    "    if df is None:\n",
    "        return None, None\n",
    "    \n",
    "    relevant_df = df[(df.F1Score > 0.01) & \n",
    "                     (df.index < 60)].copy()\n",
    "\n",
    "    #                                       Use df to compare!!\n",
    "    relevant_df = relevant_df[relevant_df.BCE < df.BCE.quantile(0.1)]\n",
    "\n",
    "    if len(relevant_df) > 0:\n",
    "        return key, relevant_df\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def garner_relevant_dictionary():\n",
    "    \n",
    "    thresholds = ['0.1', '0.2', '0.3', '0.35', '0.65', '0.7', '0.8', '0.9']\n",
    "    files_by_thresholds = agg_by_threshold(thresholds)\n",
    "\n",
    "    # Not sure if this should be default_dict\n",
    "    so_far = defaultdict(list)\n",
    "    for files in files_by_thresholds:\n",
    "        for file in files:\n",
    "            key, df = get_relevant_key_df(file)\n",
    "            if key is not None:\n",
    "                so_far[key].append(df)\n",
    "    \n",
    "    return so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = garner_relevant_dictionary()\n",
    "# iter_my_dict = iter(my_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key block_depth=4,const_factor=2,learning_rate=0.007,linear_dim=5,percentile=0.8 has len(df) > 1\n"
     ]
    }
   ],
   "source": [
    "training_results = []\n",
    "pct = 0.8\n",
    "statement = 'percentile={}'.format(pct)\n",
    "for key, df in my_dict.items():\n",
    "    if len(df) > 1:\n",
    "        print('key {} has len(df) > 1'.format(key))\n",
    "    if statement in key : # and 'block_depth=3' in key:\n",
    "        training_results.append((key, df[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_results_sorted = sorted(training_results, \n",
    "#                                  key=lambda x: (x[1].Mean*x[1].F1Score).mean(), \n",
    "#                                  reverse=True)\n",
    "reverse = pct > 0.5\n",
    "\n",
    "training_results_sorted = sorted(training_results, \n",
    "                                 key=lambda x: (x[1].Mean).mean(), \n",
    "                                 reverse=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_depth=6,const_factor=2,learning_rate=0.007,linear_dim=3,percentile=0.8\n",
      "block_depth=4,const_factor=2,learning_rate=0.007,linear_dim=5,percentile=0.8\n",
      "block_depth=6,const_factor=2,learning_rate=0.007,linear_dim=5,percentile=0.8\n",
      "block_depth=2,const_factor=2,learning_rate=0.007,linear_dim=3,percentile=0.8\n",
      "block_depth=4,const_factor=3,learning_rate=0.007,linear_dim=3,percentile=0.8\n",
      "block_depth=4,const_factor=3,learning_rate=0.007,linear_dim=5,percentile=0.8\n"
     ]
    }
   ],
   "source": [
    "for key, result in training_results_sorted:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1Score, Mean, Precision, Stdev, Count\n",
      "0.01250, 0.01349, 0.52817, 0.03937, 1\n",
      "0.11725, 0.01048, 0.37994, 0.03550, 1\n",
      "0.12810, 0.00981, 0.41386, 0.04268, 1\n",
      "0.04828, 0.00857, 0.44283, 0.03954, 1\n",
      "0.02071, 0.00839, 0.38789, 0.03839, 2\n",
      "0.08871, 0.00082, 0.33162, 0.04307, 1\n"
     ]
    }
   ],
   "source": [
    "print('F1Score, Mean, Precision, Stdev, Count')\n",
    "for key, result in training_results_sorted:\n",
    "    print('{:.5f}, {:.5f}, {:.5f}, {:.5f}, {}'.format(\n",
    "          result.F1Score.mean(), \n",
    "          result.Mean.mean(),\n",
    "          result.Precision.mean(), \n",
    "          result.Stdev.mean(),\n",
    "          result.Mean.count(),)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_depth=6,const_factor=2,learning_rate=0.007,linear_dim=3,percentile=0.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BCE</th>\n",
       "      <th>F1Score</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Stdev</th>\n",
       "      <th>ConfusionMatrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.50618</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.56624</td>\n",
       "      <td>0.52817</td>\n",
       "      <td>0.00633</td>\n",
       "      <td>0.01349</td>\n",
       "      <td>0.03937</td>\n",
       "      <td>[228145    335  58905    375]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        BCE  F1Score  ROC_AUC  Precision   Recall     Mean    Stdev  \\\n",
       "35  0.50618   0.0125  0.56624    0.52817  0.00633  0.01349  0.03937   \n",
       "\n",
       "                  ConfusionMatrix  \n",
       "35  [228145    335  58905    375]  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = 0\n",
    "print(training_results_sorted[ranks][0])\n",
    "training_results_sorted[ranks][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
