{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- log folder exists\n"
     ]
    }
   ],
   "source": [
    "from supervised import read_csv, prepare_data, TickerDataSimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, Y = prepare_data('spy', True)\n",
    "input = pd.DataFrame(X_all.iloc[:, 1:])\n",
    "\n",
    "# Random sequence data\n",
    "# train_idx, test_idx = train_test_split(np.arange(len(input)))\n",
    "\n",
    "# Sequential data\n",
    "length = int(len(input)* 0.8)\n",
    "train_idx = np.arange(length)\n",
    "test_idx = np.arange(length, len(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2520,) (631,)\n",
      "(2520, 24) (631, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1538, 377)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = input.iloc[train_idx]\n",
    "test_df = input.iloc[test_idx]\n",
    "print(train_idx.shape, test_idx.shape)\n",
    "print(train_df.shape, test_df.shape)\n",
    "# Consider some other y transfroms...\n",
    "y_train = np.where(Y[train_idx]>0, 1, 0)\n",
    "y_test = np.where(Y[test_idx]>0, 1, 0)\n",
    "y_train.sum(), y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the ones worked well in autoencoder\n",
    "transfomer = [\n",
    "    ('Data after min-max scaling',\n",
    "        MinMaxScaler()),\n",
    "    ('Data after max-abs scaling',\n",
    "        MaxAbsScaler()),\n",
    "    ('Data after quantile transformation (uniform pdf)',\n",
    "        QuantileTransformer(output_distribution='uniform')),\n",
    "    ('Data after sample-wise L2 normalizing',\n",
    "        Normalizer()),\n",
    "]\n",
    "\n",
    "combined = FeatureUnion(transfomer)\n",
    "combined_fit = combined.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transformed = combined.transform(train_df)\n",
    "x_test_transformed = combined.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2520, 96), (631, 96))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_transformed.shape, x_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for a simple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self, input_size, final_output_size):\n",
    "        super(LogisticRegressor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(input_size, 24)\n",
    "        self.l2 = nn.Linear(24, 12)\n",
    "        self.l3 = nn.Linear(12, 6)\n",
    "        self.l4 = nn.Linear(6, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.tanh(self.l2(x))\n",
    "        x = torch.tanh(self.l3(x))\n",
    "        return torch.sigmoid(self.l4(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transformed = combined.transform(train_df)\n",
    "x_test_transformed = combined.transform(test_df)\n",
    "\n",
    "spy_dataset = TickerDataSimple('spy', x_train_transformed, y_train)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dl = DataLoader(spy_dataset, \n",
    "                      num_workers=1, \n",
    "                      batch_size=BATCH_SIZE)\n",
    "\n",
    "spy_testset = TickerDataSimple('spy', x_test_transformed, y_test)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "test_dl = DataLoader(spy_testset, \n",
    "                      num_workers=1, \n",
    "                      batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.optim as optim\n",
    "\n",
    "# Each Data Points are 24 (6 * 4)\n",
    "# Transformer has 4 different ways\n",
    "model = LogisticRegressor(24 * 4, 1)\n",
    "\n",
    "criterion = nn.modules.loss.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=1e-3, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.metrics import BinaryAccuracy, Loss, Precision, Recall\n",
    "from ignite.engine import Events, \\\n",
    "                          create_supervised_trainer, \\\n",
    "                          create_supervised_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        'accuracy': BinaryAccuracy(),\n",
    "        'bce': Loss(criterion),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\"Training Results   - Epoch: {}  Avg accuracy: {:.5f} Avg loss: {:.5f}\"\n",
    "          .format(trainer.state.epoch, \n",
    "                  metrics['accuracy'], \n",
    "                  metrics['bce'],\n",
    "                 ))\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    evaluator.run(test_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\"Validation Results - Epoch: {}  Avg accuracy: {:.5f} Avg loss: {:.5f}\"\n",
    "          .format(trainer.state.epoch, \n",
    "                  metrics['accuracy'], \n",
    "                  metrics['bce'],\n",
    "                 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results   - Epoch: 1  Avg accuracy: 0.60392 Avg loss: 0.53493\n",
      "Validation Results - Epoch: 1  Avg accuracy: 0.57745 Avg loss: 0.56475\n",
      "Training Results   - Epoch: 2  Avg accuracy: 0.60319 Avg loss: 0.53171\n",
      "Validation Results - Epoch: 2  Avg accuracy: 0.57690 Avg loss: 0.56694\n",
      "Training Results   - Epoch: 3  Avg accuracy: 0.60278 Avg loss: 0.52987\n",
      "Validation Results - Epoch: 3  Avg accuracy: 0.57286 Avg loss: 0.56930\n",
      "Training Results   - Epoch: 4  Avg accuracy: 0.60218 Avg loss: 0.52844\n",
      "Validation Results - Epoch: 4  Avg accuracy: 0.57251 Avg loss: 0.57101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7fed8196ab70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_dl, max_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
