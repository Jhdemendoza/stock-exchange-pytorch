{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import datetime\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_df_test_df(ticker):\n",
    "    \n",
    "    def concat_and_return_csvs(original_df, ticker_files):\n",
    "        for item in ticker_files[1:]:\n",
    "            this_df = pd.read_csv(data_path+item)\n",
    "            original_df = pd.concat([original_df, this_df])\n",
    "        return original_df\n",
    "    \n",
    "    data_path = 'data/daily_data/'\n",
    "    ticker_files = [item for item in os.listdir(data_path) if ticker in item.split('_')]\n",
    "    ticker_files.sort()\n",
    "    \n",
    "    split_idx = int(len(ticker_files) * 0.8)\n",
    "    train_ticker_files, test_ticker_files = ticker_files[:split_idx], ticker_files[split_idx:]\n",
    "\n",
    "    train_df = pd.read_csv(data_path+train_ticker_files[0])\n",
    "    train_df = concat_and_return_csvs(train_df, train_ticker_files)\n",
    "    \n",
    "    test_df = pd.read_csv(data_path+test_ticker_files[0])\n",
    "    test_df = concat_and_return_csvs(test_df, test_ticker_files)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_minute_data(df):\n",
    "    cols = df.columns.tolist()\n",
    "    cols_to_drop = cols[:4] + ['label', 'changeOverTime', 'close', 'high', \n",
    "                               'low', 'marketAverage', 'marketClose', \n",
    "                               'marketOpen', 'volume', 'numberOfTrades', \n",
    "                               'notional', 'open', 'marketChangeOverTime']\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    # necessary\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    idx_to_drop = df.index[df.marketNotional == 0.0]\n",
    "    df.drop(idx_to_drop, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df.date = df.date.map(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d'))\n",
    "    df['weekday'] = df.date.map(lambda x: str(x.weekday()))\n",
    "    df['month']   = df.date.map(lambda x: str(x.month))\n",
    "    \n",
    "    df.minute = df.minute.map(lambda x: datetime.datetime.strptime(x, '%H:%M'))\n",
    "    df['hour'] = df.minute.map(lambda x: str(x.hour))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_categoric(df):\n",
    "    numeric_cols, categorical_cols = [], []\n",
    "\n",
    "    for col in df:\n",
    "        if np.issubdtype(df[col].dtype, np.number):\n",
    "            numeric_cols += [col]\n",
    "        else:\n",
    "            categorical_cols += [col]\n",
    "    \n",
    "    return numeric_cols, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_dataframe(df, numeric_columns):\n",
    "    '''\n",
    "    log numerical columns, then return deltas\n",
    "    '''\n",
    "    \n",
    "    MAX_SHIFT_BACWARD, MAX_SHIFT_FORWARD = -20, 20\n",
    "    added_columns = []\n",
    "    for shift in [MAX_SHIFT_BACWARD, -10, -5, 3, 5, 10, MAX_SHIFT_FORWARD]:\n",
    "        for col in numeric_columns:\n",
    "            new_col_name = col + '_' + str(shift)\n",
    "            df[new_col_name] = df[col].shift(shift)\n",
    "            added_columns += [new_col_name]\n",
    "\n",
    "    df[numeric_columns+added_columns] = df[numeric_columns+added_columns].apply(np.log)\n",
    "    \n",
    "    # for lookbacks\n",
    "    for new_col in added_columns:\n",
    "        original_col, added_part = new_col.split('_')\n",
    "        df[new_col] = df[new_col] - df[original_col] if '-' in added_part else \\\n",
    "                      df[original_col] - df[new_col]\n",
    "\n",
    "    # for today\n",
    "    # This line is necessary\n",
    "    temp = df[numeric_columns] - df[numeric_columns].shift(1)\n",
    "    df[numeric_columns] = temp\n",
    "    \n",
    "    assert (df.index == np.arange(len(df))).all()\n",
    "    df.drop(df.index[list(range(MAX_SHIFT_FORWARD))], axis=0, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    #                            negative max_shift_back...\n",
    "    df.drop(index=list(range(len(df)+MAX_SHIFT_BACWARD, len(df))), inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(ticker):\n",
    "    train_df, test_df = train_df_test_df(ticker)\n",
    "    \n",
    "#     train_df, test_df = list(map(lambda x: get_processed_minute_data(x), \n",
    "#                                  (train_df, test_df)))\n",
    "    train_df = get_processed_minute_data(train_df)\n",
    "    test_df  = get_processed_minute_data(test_df)\n",
    "    \n",
    "    numeric_cols, categoric_cols = get_numeric_categoric(train_df)\n",
    "    # This is for the time being...\n",
    "    categoric_cols = ['weekday', 'month', 'hour']\n",
    "    \n",
    "    train_df = delta_dataframe(train_df, numeric_cols)\n",
    "    test_df  = delta_dataframe(test_df,  numeric_cols)\n",
    "    \n",
    "    # Re-evaluate column names from the deltas\n",
    "    numeric_cols, _ = get_numeric_categoric(train_df)\n",
    "    \n",
    "    return train_df, test_df, numeric_cols, categoric_cols\n",
    "\n",
    "def get_y_cols(numeric_cols):\n",
    "    price_cols      = [item for item in numeric_cols if '-' in item]\n",
    "    interested_cols = [item for item in price_cols if 'High' in item or 'Low' in item]\n",
    "    not_interested_cols = list(set(price_cols) - set(interested_cols))\n",
    "    return interested_cols, not_interested_cols\n",
    "\n",
    "# messy code... \n",
    "train_df_original, test_df_original, numeric_cols, categoric_cols = load_dataframes('bkr.b')\n",
    "y_cols, not_interested = get_y_cols(numeric_cols)\n",
    "numeric_cols = list(set(numeric_cols) - set(y_cols) - set(not_interested))\n",
    "# Let's not do month embedding\n",
    "categoric_cols.remove('month')\n",
    "categorical_train_embeddings = train_df_original[categoric_cols].applymap(lambda x: int(x))\n",
    "categorical_test_embeddings  = test_df_original[categoric_cols].applymap(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, y_train = train_df_original[numeric_cols], train_df_original[y_cols]\n",
    "test_df, y_test   = test_df_original[numeric_cols], test_df_original[y_cols]\n",
    "y_train.drop(y_train.columns[2:], axis=1, inplace=True)\n",
    "y_test.drop( y_test.columns[2:],  axis=1, inplace=True)\n",
    "binary_y_train = (y_train>0.001).astype(np.int)\n",
    "binary_y_test  = (y_test >0.001).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the ones worked well in autoencoder\n",
    "transfomer = [\n",
    "    ('Data after min-max scaling',\n",
    "        MinMaxScaler()),\n",
    "    ('Data after max-abs scaling',\n",
    "        MaxAbsScaler()),\n",
    "    ('Data after quantile transformation (uniform pdf)',\n",
    "        QuantileTransformer(output_distribution='uniform')),\n",
    "    ('Data after sample-wise L2 normalizing',\n",
    "        Normalizer()),\n",
    "]\n",
    "\n",
    "combined = FeatureUnion(transfomer)\n",
    "combined_fit = combined.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transformed = combined.transform(train_df)\n",
    "x_test_transformed = combined.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9229, 100), (2661, 100))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_transformed.shape, x_test_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transformed = combined.transform(train_df)\n",
    "x_test_transformed = combined.transform(test_df)\n",
    "x_train_numerical_transformed = torch.from_numpy(x_train_transformed).float()\n",
    "x_test_numerical_transformed = torch.from_numpy(x_test_transformed).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_categorical_cols = torch.from_numpy(categorical_train_embeddings.values).float()\n",
    "x_train_categorical_cols[:, 1] -= 9\n",
    "x_test_categorical_cols = torch.from_numpy(categorical_test_embeddings.values).float()\n",
    "x_test_categorical_cols[:, 1] -= 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all = torch.cat([x_train_numerical_transformed, x_train_categorical_cols], dim=1)\n",
    "x_test_all = torch.cat([x_test_numerical_transformed, x_test_categorical_cols], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9229, 102]), torch.Size([2661, 102]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape, x_test_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self, input_size, final_output_size):\n",
    "        super(LogisticRegressor, self).__init__()\n",
    "        self.e1 = nn.Embedding(5, 2)\n",
    "        self.e2 = nn.Embedding(7, 2)\n",
    "        # +2 for embedding... 2+2-2\n",
    "        self.l1 = nn.Linear(input_size+2, 64)\n",
    "        self.l2 = nn.Linear(64, 32)\n",
    "        self.l3 = nn.Linear(32, 16)\n",
    "        self.l4 = nn.Linear(16, final_output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, = x[:, -2].long(), x[:, -1].long()\n",
    "        x1 = self.e1(x1)\n",
    "        x2 = self.e2(x2)\n",
    "        \n",
    "        x = torch.cat([x[:, :-2], x1, x2], dim=1)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.tanh(self.l2(x))\n",
    "        x = torch.tanh(self.l3(x))\n",
    "        return torch.sigmoid(self.l4(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TickerDataSimple(Dataset):\n",
    "    def __init__(self, ticker, x, y):\n",
    "        '''\n",
    "        :param ticker: string\n",
    "        :param x: np.array of x\n",
    "        :param y: np.array of y\n",
    "        '''\n",
    "        self.ticker = ticker\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(torch.nn.Module):\n",
    "    '''\n",
    "    Implement Focal Loss\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CustomLoss,self).__init__()\n",
    "        \n",
    "    def forward(self, y_pred, y_target):\n",
    "        y_pred = y_pred.flatten()\n",
    "        y_target = y_target.flatten()\n",
    "        \n",
    "        def log_p(pred, target):\n",
    "            return -((1-pred) * torch.log2(pred) * target)\n",
    "        \n",
    "        return (log_p(y_pred, y_target) + log_p(1-y_pred, 1-y_target)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.optim as optim\n",
    "\n",
    "# Each Data Points are 24 (6 * 4)\n",
    "# Transformer has 4 different ways\n",
    "model = LogisticRegressor(x_train_all.shape[1], y_train.shape[1])\n",
    "\n",
    "criterion = CustomLoss()\n",
    "# criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=5e-3, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_dataset = TickerDataSimple('spy', x_train_all, \n",
    "                               torch.from_numpy(binary_y_train.values).float())\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dl = DataLoader(spy_dataset, \n",
    "                      num_workers=1, \n",
    "                      batch_size=BATCH_SIZE)\n",
    "\n",
    "spy_testset = TickerDataSimple('spy', x_test_all, \n",
    "                               torch.from_numpy(binary_y_test.values).float())\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "test_dl = DataLoader(spy_testset, \n",
    "                      num_workers=1, \n",
    "                      batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.metrics import BinaryAccuracy, Loss, Precision, Recall\n",
    "from ignite.engine import Events, \\\n",
    "                          create_supervised_trainer, \\\n",
    "                          create_supervised_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sk_metrics\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_train_dl = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iter_train_dl)\n",
    "# _out = model(x)\n",
    "# _out = _out.flatten()\n",
    "# y    = y.flatten()\n",
    "# _zero_one = _out > 0.5\n",
    "# print('f1_score: {}'.format(sk_metrics.f1_score(_zero_one.detach().numpy(), y)))\n",
    "# print('accuracy_score: {}'.format(sk_metrics.accuracy_score(_zero_one, y)))\n",
    "# print('roc_auc_score: {}'.format(sk_metrics.roc_auc_score(y, _out.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import Accuracy\n",
    "from functools import partial\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from ignite.metrics import EpochMetric\n",
    "\n",
    "\n",
    "def sk_metric_fn(y_preds, y_targets, sk_metrics, activation=None):\n",
    "    y_true = y_targets.flatten().numpy()\n",
    "    y_pred = y_preds.flatten().numpy()\n",
    "    if activation is not None:\n",
    "        y_preds = activation(y_preds)\n",
    "    \n",
    "    return sk_metrics(y_true, y_pred)\n",
    "\n",
    "class ROC_AUC(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(ROC_AUC, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.roc_auc_score, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n",
    "\n",
    "class F1_Score(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(F1_Score, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.f1_score, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n",
    "\n",
    "class BinaryAccuracy(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(BinaryAccuracy, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.accuracy_score, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n",
    "\n",
    "class Precision(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(Precision, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.precision_score, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n",
    "\n",
    "class Recall(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(Recall, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.recall_score, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n",
    "\n",
    "class ConfusionMatrix(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(ConfusionMatrix, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.confusion_matrix, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_one(y_preds):\n",
    "    return y_preds > 0.5\n",
    "    \n",
    "def zero_one_transform(output):\n",
    "    return (zero_one(output[0])).long(), output[1].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.modules.loss.BCELoss()\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        'accuracy' : BinaryAccuracy(output_transform=zero_one_transform),\n",
    "        'bce':       Loss(bce_loss),\n",
    "        'f1_score' : F1_Score(output_transform=zero_one_transform),\n",
    "        'roc_auc'  : ROC_AUC(),\n",
    "        'precision': Precision(output_transform=zero_one_transform),\n",
    "        'recall'   : Recall(output_transform=zero_one_transform),\n",
    "        'conf_matrix': ConfusionMatrix(output_transform=zero_one_transform),},\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\"Training Results  - Epoch: {} Avg accuracy: {:.5f}, Avg BCE: {:.5f}, F1 Score: {:.5f}, ROC_AUC: {:.5f}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['accuracy'], \n",
    "                  metrics['bce'],\n",
    "                  metrics['f1_score'],\n",
    "                  metrics['roc_auc'],))\n",
    "    print(\"Training Results  - Epoch: {} Precision: {:.5f}, Recall: {:.5f}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['precision'], \n",
    "                  metrics['recall'],))\n",
    "    print(\"Training Results  - Epoch: {} Confusion Matrix \\n{}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['conf_matrix'],))\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    evaluator.run(test_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\"Validation Results- Epoch: {} Avg accuracy: {:.5f}, Avg BCE: {:.5f}, F1 Score: {:.5f}, ROC_AUC: {:.5f}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['accuracy'], \n",
    "                  metrics['bce'],\n",
    "                  metrics['f1_score'],\n",
    "                  metrics['roc_auc'],))\n",
    "    print(\"Validation Results- Epoch: {} Precision: {:.5f}, Recall: {:.5f}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['precision'],\n",
    "                  metrics['recall'],))\n",
    "    print(\"Validation Results- Epoch: {} Confusion Matrix: \\n{}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['conf_matrix'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results  - Epoch: 1 Avg accuracy: 0.60657, Avg BCE: 0.67190, F1 Score: 0.00000, ROC_AUC: 0.52066\n",
      "Training Results  - Epoch: 1 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 1 Confusion Matrix \n",
      "[[11196     0]\n",
      " [ 7262     0]]\n",
      "Validation Results- Epoch: 1 Avg accuracy: 0.64908, Avg BCE: 0.65913, F1 Score: 0.00000, ROC_AUC: 0.51941\n",
      "Validation Results- Epoch: 1 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 1 Confusion Matrix: \n",
      "[[2952    0]\n",
      " [1596    0]]\n",
      "Training Results  - Epoch: 2 Avg accuracy: 0.60657, Avg BCE: 0.67396, F1 Score: 0.00000, ROC_AUC: 0.52074\n",
      "Training Results  - Epoch: 2 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 2 Confusion Matrix \n",
      "[[11196     0]\n",
      " [ 7262     0]]\n",
      "Validation Results- Epoch: 2 Avg accuracy: 0.64908, Avg BCE: 0.66343, F1 Score: 0.00000, ROC_AUC: 0.51897\n",
      "Validation Results- Epoch: 2 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 2 Confusion Matrix: \n",
      "[[2952    0]\n",
      " [1596    0]]\n",
      "Training Results  - Epoch: 3 Avg accuracy: 0.60657, Avg BCE: 0.66953, F1 Score: 0.00000, ROC_AUC: 0.52049\n",
      "Training Results  - Epoch: 3 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 3 Confusion Matrix \n",
      "[[11196     0]\n",
      " [ 7262     0]]\n",
      "Validation Results- Epoch: 3 Avg accuracy: 0.64908, Avg BCE: 0.65301, F1 Score: 0.00000, ROC_AUC: 0.48610\n",
      "Validation Results- Epoch: 3 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 3 Confusion Matrix: \n",
      "[[2952    0]\n",
      " [1596    0]]\n",
      "Training Results  - Epoch: 4 Avg accuracy: 0.60657, Avg BCE: 0.66944, F1 Score: 0.00000, ROC_AUC: 0.53074\n",
      "Training Results  - Epoch: 4 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 4 Confusion Matrix \n",
      "[[11196     0]\n",
      " [ 7262     0]]\n",
      "Validation Results- Epoch: 4 Avg accuracy: 0.64908, Avg BCE: 0.65715, F1 Score: 0.00000, ROC_AUC: 0.49018\n",
      "Validation Results- Epoch: 4 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 4 Confusion Matrix: \n",
      "[[2952    0]\n",
      " [1596    0]]\n",
      "Training Results  - Epoch: 5 Avg accuracy: 0.60657, Avg BCE: 0.67115, F1 Score: 0.00000, ROC_AUC: 0.52463\n",
      "Training Results  - Epoch: 5 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 5 Confusion Matrix \n",
      "[[11196     0]\n",
      " [ 7262     0]]\n",
      "Validation Results- Epoch: 5 Avg accuracy: 0.64908, Avg BCE: 0.66043, F1 Score: 0.00000, ROC_AUC: 0.49613\n",
      "Validation Results- Epoch: 5 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 5 Confusion Matrix: \n",
      "[[2952    0]\n",
      " [1596    0]]\n",
      "Training Results  - Epoch: 6 Avg accuracy: 0.60657, Avg BCE: 0.67427, F1 Score: 0.00000, ROC_AUC: 0.51005\n",
      "Training Results  - Epoch: 6 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 6 Confusion Matrix \n",
      "[[11196     0]\n",
      " [ 7262     0]]\n",
      "Validation Results- Epoch: 6 Avg accuracy: 0.64908, Avg BCE: 0.66423, F1 Score: 0.00000, ROC_AUC: 0.50545\n",
      "Validation Results- Epoch: 6 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 6 Confusion Matrix: \n",
      "[[2952    0]\n",
      " [1596    0]]\n",
      "Training Results  - Epoch: 7 Avg accuracy: 0.60689, Avg BCE: 0.67347, F1 Score: 0.00165, ROC_AUC: 0.51636\n",
      "Training Results  - Epoch: 7 Precision: 1.00000, Recall: 0.00083\n",
      "Training Results  - Epoch: 7 Confusion Matrix \n",
      "[[11196     0]\n",
      " [ 7256     6]]\n",
      "Validation Results- Epoch: 7 Avg accuracy: 0.64952, Avg BCE: 0.66283, F1 Score: 0.00250, ROC_AUC: 0.50321\n",
      "Validation Results- Epoch: 7 Precision: 1.00000, Recall: 0.00125\n",
      "Validation Results- Epoch: 7 Confusion Matrix: \n",
      "[[2952    0]\n",
      " [1594    2]]\n",
      "Training Results  - Epoch: 8 Avg accuracy: 0.60830, Avg BCE: 0.67331, F1 Score: 0.01525, ROC_AUC: 0.51198\n",
      "Training Results  - Epoch: 8 Precision: 0.70000, Recall: 0.00771\n",
      "Training Results  - Epoch: 8 Confusion Matrix \n",
      "[[11172    24]\n",
      " [ 7206    56]]\n",
      "Validation Results- Epoch: 8 Avg accuracy: 0.64974, Avg BCE: 0.66247, F1 Score: 0.01727, ROC_AUC: 0.50732\n",
      "Validation Results- Epoch: 8 Precision: 0.56000, Recall: 0.00877\n",
      "Validation Results- Epoch: 8 Confusion Matrix: \n",
      "[[2941   11]\n",
      " [1582   14]]\n",
      "Training Results  - Epoch: 9 Avg accuracy: 0.60852, Avg BCE: 0.66994, F1 Score: 0.03679, ROC_AUC: 0.52830\n",
      "Training Results  - Epoch: 9 Precision: 0.57500, Recall: 0.01900\n",
      "Training Results  - Epoch: 9 Confusion Matrix \n",
      "[[11094   102]\n",
      " [ 7124   138]]\n",
      "Validation Results- Epoch: 9 Avg accuracy: 0.64974, Avg BCE: 0.65734, F1 Score: 0.04209, ROC_AUC: 0.50769\n",
      "Validation Results- Epoch: 9 Precision: 0.52239, Recall: 0.02193\n",
      "Validation Results- Epoch: 9 Confusion Matrix: \n",
      "[[2920   32]\n",
      " [1561   35]]\n",
      "Training Results  - Epoch: 10 Avg accuracy: 0.60705, Avg BCE: 0.67384, F1 Score: 0.00248, ROC_AUC: 0.52768\n",
      "Training Results  - Epoch: 10 Precision: 1.00000, Recall: 0.00124\n",
      "Training Results  - Epoch: 10 Confusion Matrix \n",
      "[[11196     0]\n",
      " [ 7253     9]]\n",
      "Validation Results- Epoch: 10 Avg accuracy: 0.64930, Avg BCE: 0.66404, F1 Score: 0.00375, ROC_AUC: 0.50583\n",
      "Validation Results- Epoch: 10 Precision: 0.60000, Recall: 0.00188\n",
      "Validation Results- Epoch: 10 Confusion Matrix: \n",
      "[[2950    2]\n",
      " [1593    3]]\n",
      "Training Results  - Epoch: 11 Avg accuracy: 0.60684, Avg BCE: 0.67467, F1 Score: 0.00138, ROC_AUC: 0.52007\n",
      "Training Results  - Epoch: 11 Precision: 1.00000, Recall: 0.00069\n",
      "Training Results  - Epoch: 11 Confusion Matrix \n",
      "[[11196     0]\n",
      " [ 7257     5]]\n",
      "Validation Results- Epoch: 11 Avg accuracy: 0.64952, Avg BCE: 0.66502, F1 Score: 0.00250, ROC_AUC: 0.50797\n",
      "Validation Results- Epoch: 11 Precision: 1.00000, Recall: 0.00125\n",
      "Validation Results- Epoch: 11 Confusion Matrix: \n",
      "[[2952    0]\n",
      " [1594    2]]\n",
      "Training Results  - Epoch: 12 Avg accuracy: 0.60749, Avg BCE: 0.66984, F1 Score: 0.00604, ROC_AUC: 0.51811\n",
      "Training Results  - Epoch: 12 Precision: 0.81481, Recall: 0.00303\n",
      "Training Results  - Epoch: 12 Confusion Matrix \n",
      "[[11191     5]\n",
      " [ 7240    22]]\n",
      "Validation Results- Epoch: 12 Avg accuracy: 0.64754, Avg BCE: 0.65342, F1 Score: 0.00373, ROC_AUC: 0.49561\n",
      "Validation Results- Epoch: 12 Precision: 0.23077, Recall: 0.00188\n",
      "Validation Results- Epoch: 12 Confusion Matrix: \n",
      "[[2942   10]\n",
      " [1593    3]]\n",
      "Training Results  - Epoch: 13 Avg accuracy: 0.60900, Avg BCE: 0.66788, F1 Score: 0.04297, ROC_AUC: 0.53787\n",
      "Training Results  - Epoch: 13 Precision: 0.58065, Recall: 0.02231\n",
      "Training Results  - Epoch: 13 Confusion Matrix \n",
      "[[11079   117]\n",
      " [ 7100   162]]\n",
      "Validation Results- Epoch: 13 Avg accuracy: 0.64996, Avg BCE: 0.65109, F1 Score: 0.05125, ROC_AUC: 0.49981\n",
      "Validation Results- Epoch: 13 Precision: 0.52439, Recall: 0.02694\n",
      "Validation Results- Epoch: 13 Confusion Matrix: \n",
      "[[2913   39]\n",
      " [1553   43]]\n",
      "Training Results  - Epoch: 14 Avg accuracy: 0.60922, Avg BCE: 0.66615, F1 Score: 0.05527, ROC_AUC: 0.55115\n",
      "Training Results  - Epoch: 14 Precision: 0.56568, Recall: 0.02906\n",
      "Training Results  - Epoch: 14 Confusion Matrix \n",
      "[[11034   162]\n",
      " [ 7051   211]]\n",
      "Validation Results- Epoch: 14 Avg accuracy: 0.64842, Avg BCE: 0.65057, F1 Score: 0.05216, ROC_AUC: 0.51312\n",
      "Validation Results- Epoch: 14 Precision: 0.48352, Recall: 0.02757\n",
      "Validation Results- Epoch: 14 Confusion Matrix: \n",
      "[[2905   47]\n",
      " [1552   44]]\n",
      "Training Results  - Epoch: 15 Avg accuracy: 0.61030, Avg BCE: 0.66543, F1 Score: 0.04615, ROC_AUC: 0.55338\n",
      "Training Results  - Epoch: 15 Precision: 0.62366, Recall: 0.02396\n",
      "Training Results  - Epoch: 15 Confusion Matrix \n",
      "[[11091   105]\n",
      " [ 7088   174]]\n",
      "Validation Results- Epoch: 15 Avg accuracy: 0.64754, Avg BCE: 0.65304, F1 Score: 0.03954, ROC_AUC: 0.51739\n",
      "Validation Results- Epoch: 15 Precision: 0.45205, Recall: 0.02068\n",
      "Validation Results- Epoch: 15 Confusion Matrix: \n",
      "[[2912   40]\n",
      " [1563   33]]\n",
      "Training Results  - Epoch: 16 Avg accuracy: 0.60982, Avg BCE: 0.66782, F1 Score: 0.03768, ROC_AUC: 0.55058\n",
      "Training Results  - Epoch: 16 Precision: 0.63514, Recall: 0.01942\n",
      "Training Results  - Epoch: 16 Confusion Matrix \n",
      "[[11115    81]\n",
      " [ 7121   141]]\n",
      "Validation Results- Epoch: 16 Avg accuracy: 0.64930, Avg BCE: 0.65744, F1 Score: 0.03742, ROC_AUC: 0.51053\n",
      "Validation Results- Epoch: 16 Precision: 0.50820, Recall: 0.01942\n",
      "Validation Results- Epoch: 16 Confusion Matrix: \n",
      "[[2922   30]\n",
      " [1565   31]]\n",
      "Training Results  - Epoch: 17 Avg accuracy: 0.60722, Avg BCE: 0.67230, F1 Score: 0.00357, ROC_AUC: 0.53500\n",
      "Training Results  - Epoch: 17 Precision: 0.92857, Recall: 0.00179\n",
      "Training Results  - Epoch: 17 Confusion Matrix \n",
      "[[11195     1]\n",
      " [ 7249    13]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results- Epoch: 17 Avg accuracy: 0.64908, Avg BCE: 0.66294, F1 Score: 0.00499, ROC_AUC: 0.51031\n",
      "Validation Results- Epoch: 17 Precision: 0.50000, Recall: 0.00251\n",
      "Validation Results- Epoch: 17 Confusion Matrix: \n",
      "[[2948    4]\n",
      " [1592    4]]\n",
      "Training Results  - Epoch: 18 Avg accuracy: 0.60657, Avg BCE: 0.67495, F1 Score: 0.00000, ROC_AUC: 0.52782\n",
      "Training Results  - Epoch: 18 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 18 Confusion Matrix \n",
      "[[11196     0]\n",
      " [ 7262     0]]\n",
      "Validation Results- Epoch: 18 Avg accuracy: 0.64908, Avg BCE: 0.66527, F1 Score: 0.00000, ROC_AUC: 0.51033\n",
      "Validation Results- Epoch: 18 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 18 Confusion Matrix: \n",
      "[[2952    0]\n",
      " [1596    0]]\n",
      "Training Results  - Epoch: 19 Avg accuracy: 0.60781, Avg BCE: 0.67228, F1 Score: 0.00795, ROC_AUC: 0.54670\n",
      "Training Results  - Epoch: 19 Precision: 0.82857, Recall: 0.00399\n",
      "Training Results  - Epoch: 19 Confusion Matrix \n",
      "[[11190     6]\n",
      " [ 7233    29]]\n",
      "Validation Results- Epoch: 19 Avg accuracy: 0.64952, Avg BCE: 0.66269, F1 Score: 0.01361, ROC_AUC: 0.53199\n",
      "Validation Results- Epoch: 19 Precision: 0.55000, Recall: 0.00689\n",
      "Validation Results- Epoch: 19 Confusion Matrix: \n",
      "[[2943    9]\n",
      " [1585   11]]\n",
      "Training Results  - Epoch: 20 Avg accuracy: 0.60716, Avg BCE: 0.67286, F1 Score: 0.00385, ROC_AUC: 0.53499\n",
      "Training Results  - Epoch: 20 Precision: 0.82353, Recall: 0.00193\n",
      "Training Results  - Epoch: 20 Confusion Matrix \n",
      "[[11193     3]\n",
      " [ 7248    14]]\n",
      "Validation Results- Epoch: 20 Avg accuracy: 0.64930, Avg BCE: 0.66319, F1 Score: 0.00747, ROC_AUC: 0.52263\n",
      "Validation Results- Epoch: 20 Precision: 0.54545, Recall: 0.00376\n",
      "Validation Results- Epoch: 20 Confusion Matrix: \n",
      "[[2947    5]\n",
      " [1590    6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7fbe945c4e10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_dl, max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.state.iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clone_tf]",
   "language": "python",
   "name": "conda-env-clone_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
