{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import datetime\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_df_test_df(ticker):\n",
    "    \n",
    "    def concat_and_return_csvs(original_df, ticker_files):\n",
    "        for item in ticker_files[1:]:\n",
    "            this_df = pd.read_csv(data_path+item)\n",
    "            original_df = pd.concat([original_df, this_df])\n",
    "        return original_df\n",
    "    \n",
    "    data_path = 'data/daily_data/'\n",
    "    ticker_files = [item for item in os.listdir(data_path) if ticker in item.split('_')]\n",
    "    ticker_files.sort()\n",
    "    \n",
    "    split_idx = int(len(ticker_files) * 0.8)\n",
    "    train_ticker_files, test_ticker_files = ticker_files[:split_idx], ticker_files[split_idx:]\n",
    "\n",
    "    train_df = pd.read_csv(data_path+train_ticker_files[0])\n",
    "    train_df = concat_and_return_csvs(train_df, train_ticker_files)\n",
    "    \n",
    "    test_df = pd.read_csv(data_path+test_ticker_files[0])\n",
    "    test_df = concat_and_return_csvs(test_df, test_ticker_files)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_minute_data(df):\n",
    "    cols = df.columns.tolist()\n",
    "    cols_to_drop = cols[:4] + ['label', 'changeOverTime', 'close', 'high', \n",
    "                               'low', 'marketAverage', 'marketClose', \n",
    "                               'marketOpen', 'volume', 'numberOfTrades', \n",
    "                               'notional', 'open', 'marketChangeOverTime']\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    # necessary\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    idx_to_drop = df.index[df.marketNotional == 0.0]\n",
    "    df.drop(idx_to_drop, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df.date = df.date.map(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d'))\n",
    "    df['weekday'] = df.date.map(lambda x: str(x.weekday()))\n",
    "    df['month']   = df.date.map(lambda x: str(x.month))\n",
    "    \n",
    "    df.minute = df.minute.map(lambda x: datetime.datetime.strptime(x, '%H:%M'))\n",
    "    df['hour'] = df.minute.map(lambda x: str(x.hour))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_categoric(df):\n",
    "    numeric_cols, categorical_cols = [], []\n",
    "\n",
    "    for col in df:\n",
    "        if np.issubdtype(df[col].dtype, np.number):\n",
    "            numeric_cols += [col]\n",
    "        else:\n",
    "            categorical_cols += [col]\n",
    "    \n",
    "    return numeric_cols, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_dataframe(df, numeric_columns):\n",
    "    '''\n",
    "    log numerical columns, then return deltas\n",
    "    '''\n",
    "    \n",
    "    MAX_SHIFT_BACWARD, MAX_SHIFT_FORWARD = -20, 20\n",
    "    added_columns = []\n",
    "    for shift in [MAX_SHIFT_BACWARD, -10, -5, 3, 5, 10, MAX_SHIFT_FORWARD]:\n",
    "        for col in numeric_columns:\n",
    "            new_col_name = col + '_' + str(shift)\n",
    "            df[new_col_name] = df[col].shift(shift)\n",
    "            added_columns += [new_col_name]\n",
    "\n",
    "    df[numeric_columns+added_columns] = df[numeric_columns+added_columns].apply(np.log)\n",
    "    \n",
    "    # for lookbacks\n",
    "    for new_col in added_columns:\n",
    "        original_col, added_part = new_col.split('_')\n",
    "        df[new_col] = df[new_col] - df[original_col] if '-' in added_part else \\\n",
    "                      df[original_col] - df[new_col]\n",
    "\n",
    "    # for today\n",
    "    # This line is necessary\n",
    "    temp = df[numeric_columns] - df[numeric_columns].shift(1)\n",
    "    df[numeric_columns] = temp\n",
    "    \n",
    "    assert (df.index == np.arange(len(df))).all()\n",
    "    df.drop(df.index[list(range(MAX_SHIFT_FORWARD))], axis=0, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    #                            negative max_shift_back...\n",
    "    df.drop(index=list(range(len(df)+MAX_SHIFT_BACWARD, len(df))), inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(ticker):\n",
    "    train_df, test_df = train_df_test_df(ticker)\n",
    "    \n",
    "#     train_df, test_df = list(map(lambda x: get_processed_minute_data(x), \n",
    "#                                  (train_df, test_df)))\n",
    "    train_df = get_processed_minute_data(train_df)\n",
    "    test_df  = get_processed_minute_data(test_df)\n",
    "    \n",
    "    numeric_cols, categoric_cols = get_numeric_categoric(train_df)\n",
    "    # This is for the time being...\n",
    "    categoric_cols = ['weekday', 'month', 'hour']\n",
    "    \n",
    "    train_df = delta_dataframe(train_df, numeric_cols)\n",
    "    test_df  = delta_dataframe(test_df,  numeric_cols)\n",
    "    \n",
    "    # Re-evaluate column names from the deltas\n",
    "    numeric_cols, _ = get_numeric_categoric(train_df)\n",
    "    \n",
    "    return train_df, test_df, numeric_cols, categoric_cols\n",
    "\n",
    "def get_y_cols(numeric_cols):\n",
    "    price_cols      = [item for item in numeric_cols if '-' in item]\n",
    "    interested_cols = [item for item in price_cols if 'High' in item or 'Low' in item]\n",
    "    not_interested_cols = list(set(price_cols) - set(interested_cols))\n",
    "    return interested_cols, not_interested_cols\n",
    "\n",
    "# messy code... \n",
    "train_df_original, test_df_original, numeric_cols, categoric_cols = load_dataframes('ual')\n",
    "y_cols, not_interested = get_y_cols(numeric_cols)\n",
    "numeric_cols = list(set(numeric_cols) - set(y_cols) - set(not_interested))\n",
    "# Let's not do month embedding\n",
    "categoric_cols.remove('month')\n",
    "categorical_train_embeddings = train_df_original[categoric_cols].applymap(lambda x: int(x))\n",
    "categorical_test_embeddings  = test_df_original[categoric_cols].applymap(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, y_train = train_df_original[numeric_cols], train_df_original[y_cols]\n",
    "test_df, y_test   = test_df_original[numeric_cols], test_df_original[y_cols]\n",
    "y_train.drop(y_train.columns[2:], axis=1, inplace=True)\n",
    "y_test.drop( y_test.columns[2:],  axis=1, inplace=True)\n",
    "binary_y_train = (y_train>0.001).astype(np.int)\n",
    "binary_y_test  = (y_test >0.001).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the ones worked well in autoencoder\n",
    "transfomer = [\n",
    "    ('Data after min-max scaling',\n",
    "        MinMaxScaler()),\n",
    "    ('Data after max-abs scaling',\n",
    "        MaxAbsScaler()),\n",
    "    ('Data after quantile transformation (uniform pdf)',\n",
    "        QuantileTransformer(output_distribution='uniform')),\n",
    "    ('Data after sample-wise L2 normalizing',\n",
    "        Normalizer()),\n",
    "]\n",
    "\n",
    "combined = FeatureUnion(transfomer)\n",
    "combined_fit = combined.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transformed = combined.transform(train_df)\n",
    "x_test_transformed = combined.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10100, 100), (2682, 100))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_transformed.shape, x_test_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transformed = combined.transform(train_df)\n",
    "x_test_transformed = combined.transform(test_df)\n",
    "x_train_numerical_transformed = torch.from_numpy(x_train_transformed).float()\n",
    "x_test_numerical_transformed = torch.from_numpy(x_test_transformed).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_categorical_cols = torch.from_numpy(categorical_train_embeddings.values).float()\n",
    "x_train_categorical_cols[:, 1] -= 9\n",
    "x_test_categorical_cols = torch.from_numpy(categorical_test_embeddings.values).float()\n",
    "x_test_categorical_cols[:, 1] -= 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all = torch.cat([x_train_numerical_transformed, x_train_categorical_cols], dim=1)\n",
    "x_test_all = torch.cat([x_test_numerical_transformed, x_test_categorical_cols], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10100, 102]), torch.Size([2682, 102]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape, x_test_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5629e+23, 3.0962e-41],\n",
       "        [4.4625e+23, 3.0962e-41]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self, input_size, final_output_size):\n",
    "        super(LogisticRegressor, self).__init__()\n",
    "        self.e1 = nn.Embedding(5, 2)\n",
    "        self.e2 = nn.Embedding(7, 2)\n",
    "        # +2 for embedding... 2+2-2\n",
    "        self.l1 = nn.Linear(input_size+2, 64)\n",
    "        self.l2 = nn.Linear(64, 32)\n",
    "        self.l3 = nn.Linear(32, 16)\n",
    "        self.l4 = nn.Linear(16, final_output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, = x[:, -2].long(), x[:, -1].long()\n",
    "        x1 = self.e1(x1)\n",
    "        x2 = self.e2(x2)\n",
    "        \n",
    "        x = torch.cat([x[:, :-2], x1, x2], dim=1)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.tanh(self.l2(x))\n",
    "        x = torch.tanh(self.l3(x))\n",
    "        return torch.sigmoid(self.l4(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TickerDataSimple(Dataset):\n",
    "    def __init__(self, ticker, x, y):\n",
    "        '''\n",
    "        :param ticker: string\n",
    "        :param x: np.array of x\n",
    "        :param y: np.array of y\n",
    "        '''\n",
    "        self.ticker = ticker\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(torch.nn.Module):\n",
    "    '''\n",
    "    Implement Focal Loss\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CustomLoss,self).__init__()\n",
    "        \n",
    "    def forward(self, y_pred, y_target):\n",
    "        y_pred = y_pred.flatten()\n",
    "        y_target = y_target.flatten()\n",
    "        \n",
    "        def log_p(pred, target):\n",
    "            return -((1-pred) * torch.log2(pred) * target)\n",
    "        \n",
    "        return (log_p(y_pred, y_target) + log_p(1-y_pred, 1-y_target)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.optim as optim\n",
    "\n",
    "# Each Data Points are 24 (6 * 4)\n",
    "# Transformer has 4 different ways\n",
    "model = LogisticRegressor(x_train_all.shape[1], y_train.shape[1])\n",
    "\n",
    "criterion = CustomLoss()\n",
    "# criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=5e-3, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_dataset = TickerDataSimple('spy', x_train_all, \n",
    "                               torch.from_numpy(binary_y_train.values).float())\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dl = DataLoader(spy_dataset, \n",
    "                      num_workers=1, \n",
    "                      batch_size=BATCH_SIZE)\n",
    "\n",
    "spy_testset = TickerDataSimple('spy', x_test_all, \n",
    "                               torch.from_numpy(binary_y_test.values).float())\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "test_dl = DataLoader(spy_testset, \n",
    "                      num_workers=1, \n",
    "                      batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.metrics import BinaryAccuracy, Loss, Precision, Recall\n",
    "from ignite.engine import Events, \\\n",
    "                          create_supervised_trainer, \\\n",
    "                          create_supervised_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sk_metrics\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_train_dl = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iter_train_dl)\n",
    "# _out = model(x)\n",
    "# _out = _out.flatten()\n",
    "# y    = y.flatten()\n",
    "# _zero_one = _out > 0.5\n",
    "# print('f1_score: {}'.format(sk_metrics.f1_score(_zero_one.detach().numpy(), y)))\n",
    "# print('accuracy_score: {}'.format(sk_metrics.accuracy_score(_zero_one, y)))\n",
    "# print('roc_auc_score: {}'.format(sk_metrics.roc_auc_score(y, _out.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import Accuracy\n",
    "from functools import partial\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from ignite.metrics import EpochMetric\n",
    "\n",
    "\n",
    "def sk_metric_fn(y_preds, y_targets, sk_metrics, activation=None):\n",
    "    y_true = y_targets.flatten().numpy()\n",
    "    y_pred = y_preds.flatten().numpy()\n",
    "    if activation is not None:\n",
    "        y_preds = activation(y_preds)\n",
    "    \n",
    "    return sk_metrics(y_true, y_pred)\n",
    "\n",
    "class ROC_AUC(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(ROC_AUC, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.roc_auc_score, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n",
    "\n",
    "class F1_Score(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(F1_Score, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.f1_score, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n",
    "\n",
    "class BinaryAccuracy(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(BinaryAccuracy, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.accuracy_score, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n",
    "\n",
    "class Precision(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(Precision, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.precision_score, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n",
    "\n",
    "class Recall(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(Recall, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.recall_score, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n",
    "\n",
    "class ConfusionMatrix(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(ConfusionMatrix, self).__init__(\n",
    "            partial(sk_metric_fn, \n",
    "                    sk_metrics=sk_metrics.confusion_matrix, \n",
    "                    activation=activation),\n",
    "            output_transform=output_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositiveStatistics(EpochMetric):\n",
    "    def __init__(self, activation=None, output_transform=lambda x: x):\n",
    "        super(PositiveStatistics, self).__init__(\n",
    "            self.filter_positive, output_transform=output_transform)\n",
    "    def filter_positive(self, pred, target, threshold=0.5):\n",
    "        # Invalid shape\n",
    "        if pred.shape != y_train.shape and pred.shape != y_test.shape:\n",
    "            return 0.0, -1.0\n",
    "        \n",
    "        mask          = pred.ge(threshold)\n",
    "        relevant_pred = torch.masked_select(pred, mask)\n",
    "        \n",
    "        if relevant_pred.nelement() == 0:\n",
    "            print(f'Returning from empty mask:')\n",
    "            return 0.0, -1.0\n",
    "        \n",
    "        if pred.shape == y_train.shape:\n",
    "            y_value = torch.masked_select(torch.FloatTensor(y_train.values), mask)\n",
    "        if pred.shape == y_test.shape:\n",
    "            y_value = torch.masked_select(torch.FloatTensor(y_test.values), mask)\n",
    "\n",
    "        print(f'relevant_pred : {relevant_pred}')\n",
    "        print(f'y_value       : {y_value}')\n",
    "        distribution = relevant_pred * y_value\n",
    "        \n",
    "        return distribution.mean(), distribution.std()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_one(y_preds):\n",
    "    return y_preds > 0.5\n",
    "    \n",
    "def zero_one_transform(output):\n",
    "    return (zero_one(output[0])).long(), output[1].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.modules.loss.BCELoss()\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        'accuracy'     : BinaryAccuracy(output_transform=zero_one_transform),\n",
    "        'bce'          : Loss(bce_loss),\n",
    "        'f1_score'     : F1_Score(output_transform=zero_one_transform),\n",
    "        'roc_auc'      : ROC_AUC(),\n",
    "        'precision'    : Precision(output_transform=zero_one_transform),\n",
    "        'recall'       : Recall(output_transform=zero_one_transform),\n",
    "        'conf_matrix'  : ConfusionMatrix(output_transform=zero_one_transform),\n",
    "        'positive_stat': PositiveStatistics(),\n",
    "    },\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\"Training Results  - Epoch: {} Avg accuracy: {:.5f}, Avg BCE: {:.5f}, F1 Score: {:.5f}, ROC_AUC: {:.5f}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['accuracy'], \n",
    "                  metrics['bce'],\n",
    "                  metrics['f1_score'],\n",
    "                  metrics['roc_auc'],))\n",
    "    print(\"Training Results  - Epoch: {} Precision: {:.5f}, Recall: {:.5f}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['precision'], \n",
    "                  metrics['recall'],))\n",
    "    print(\"Training Results  - Epoch: {} Confusion Matrix \\n{}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['conf_matrix'],))\n",
    "    print(\"Training Results  - Epoch: {} Pred Positive Stat: {:.5f}, {:.5f}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['positive_stat'][0],\n",
    "                  metrics['positive_stat'][1]))\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    evaluator.run(test_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\"Validation Results- Epoch: {} Avg accuracy: {:.5f}, Avg BCE: {:.5f}, F1 Score: {:.5f}, ROC_AUC: {:.5f}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['accuracy'], \n",
    "                  metrics['bce'],\n",
    "                  metrics['f1_score'],\n",
    "                  metrics['roc_auc'],))\n",
    "    print(\"Validation Results- Epoch: {} Precision: {:.5f}, Recall: {:.5f}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['precision'],\n",
    "                  metrics['recall'],))\n",
    "    print(\"Validation Results- Epoch: {} Confusion Matrix: \\n{}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['conf_matrix'],))\n",
    "    print(\"Validation Results  - Epoch: {} Pred Positive Stat: {:.5f}, {:.5f}\".format(\n",
    "                  trainer.state.epoch, \n",
    "                  metrics['positive_stat'][0],\n",
    "                  metrics['positive_stat'][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results  - Epoch: 1 Avg accuracy: 0.60277, Avg BCE: 0.67344, F1 Score: 0.00000, ROC_AUC: 0.51117\n",
      "Training Results  - Epoch: 1 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 1 Confusion Matrix \n",
      "[[12176     0]\n",
      " [ 8024     0]]\n",
      "Training Results  - Epoch: 1 Pred Positive Stat: 0.00000, -1.00000\n",
      "Validation Results- Epoch: 1 Avg accuracy: 0.62696, Avg BCE: 0.66619, F1 Score: 0.00000, ROC_AUC: 0.51051\n",
      "Validation Results- Epoch: 1 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 1 Confusion Matrix: \n",
      "[[3363    0]\n",
      " [2001    0]]\n",
      "Validation Results  - Epoch: 1 Pred Positive Stat: 0.00000, -1.00000\n",
      "Training Results  - Epoch: 2 Avg accuracy: 0.60277, Avg BCE: 0.67504, F1 Score: 0.00000, ROC_AUC: 0.52110\n",
      "Training Results  - Epoch: 2 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 2 Confusion Matrix \n",
      "[[12176     0]\n",
      " [ 8024     0]]\n",
      "Training Results  - Epoch: 2 Pred Positive Stat: 0.00000, -1.00000\n",
      "Validation Results- Epoch: 2 Avg accuracy: 0.62696, Avg BCE: 0.66906, F1 Score: 0.00000, ROC_AUC: 0.52274\n",
      "Validation Results- Epoch: 2 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 2 Confusion Matrix: \n",
      "[[3363    0]\n",
      " [2001    0]]\n",
      "Validation Results  - Epoch: 2 Pred Positive Stat: 0.00000, -1.00000\n",
      "Training Results  - Epoch: 3 Avg accuracy: 0.60277, Avg BCE: 0.67540, F1 Score: 0.00000, ROC_AUC: 0.52300\n",
      "Training Results  - Epoch: 3 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 3 Confusion Matrix \n",
      "[[12176     0]\n",
      " [ 8024     0]]\n",
      "Training Results  - Epoch: 3 Pred Positive Stat: 0.00000, -1.00000\n",
      "Validation Results- Epoch: 3 Avg accuracy: 0.62696, Avg BCE: 0.66967, F1 Score: 0.00000, ROC_AUC: 0.52639\n",
      "Validation Results- Epoch: 3 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 3 Confusion Matrix: \n",
      "[[3363    0]\n",
      " [2001    0]]\n",
      "Validation Results  - Epoch: 3 Pred Positive Stat: 0.00000, -1.00000\n",
      "Training Results  - Epoch: 4 Avg accuracy: 0.60277, Avg BCE: 0.67534, F1 Score: 0.00000, ROC_AUC: 0.52702\n",
      "Training Results  - Epoch: 4 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 4 Confusion Matrix \n",
      "[[12176     0]\n",
      " [ 8024     0]]\n",
      "Training Results  - Epoch: 4 Pred Positive Stat: 0.00000, -1.00000\n",
      "Validation Results- Epoch: 4 Avg accuracy: 0.62696, Avg BCE: 0.66981, F1 Score: 0.00000, ROC_AUC: 0.52193\n",
      "Validation Results- Epoch: 4 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 4 Confusion Matrix: \n",
      "[[3363    0]\n",
      " [2001    0]]\n",
      "Validation Results  - Epoch: 4 Pred Positive Stat: 0.00000, -1.00000\n",
      "Training Results  - Epoch: 5 Avg accuracy: 0.60277, Avg BCE: 0.67555, F1 Score: 0.00000, ROC_AUC: 0.51857\n",
      "Training Results  - Epoch: 5 Precision: 0.00000, Recall: 0.00000\n",
      "Training Results  - Epoch: 5 Confusion Matrix \n",
      "[[12176     0]\n",
      " [ 8024     0]]\n",
      "Training Results  - Epoch: 5 Pred Positive Stat: 0.00000, -1.00000\n",
      "Validation Results- Epoch: 5 Avg accuracy: 0.62696, Avg BCE: 0.66973, F1 Score: 0.00000, ROC_AUC: 0.52394\n",
      "Validation Results- Epoch: 5 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 5 Confusion Matrix: \n",
      "[[3363    0]\n",
      " [2001    0]]\n",
      "Validation Results  - Epoch: 5 Pred Positive Stat: 0.00000, -1.00000\n",
      "Training Results  - Epoch: 6 Avg accuracy: 0.60282, Avg BCE: 0.67516, F1 Score: 0.00075, ROC_AUC: 0.53417\n",
      "Training Results  - Epoch: 6 Precision: 0.60000, Recall: 0.00037\n",
      "Training Results  - Epoch: 6 Confusion Matrix \n",
      "[[12174     2]\n",
      " [ 8021     3]]\n",
      "Training Results  - Epoch: 6 Pred Positive Stat: 0.00025, 0.00302\n",
      "Validation Results- Epoch: 6 Avg accuracy: 0.62677, Avg BCE: 0.66970, F1 Score: 0.00000, ROC_AUC: 0.52816\n",
      "Validation Results- Epoch: 6 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 6 Confusion Matrix: \n",
      "[[3362    1]\n",
      " [2001    0]]\n",
      "Validation Results  - Epoch: 6 Pred Positive Stat: -0.00209, nan\n",
      "Training Results  - Epoch: 7 Avg accuracy: 0.60292, Avg BCE: 0.67639, F1 Score: 0.00075, ROC_AUC: 0.52532\n",
      "Training Results  - Epoch: 7 Precision: 1.00000, Recall: 0.00037\n",
      "Training Results  - Epoch: 7 Confusion Matrix \n",
      "[[12176     0]\n",
      " [ 8021     3]]\n",
      "Training Results  - Epoch: 7 Pred Positive Stat: 0.00240, 0.00051\n",
      "Validation Results- Epoch: 7 Avg accuracy: 0.62696, Avg BCE: 0.67101, F1 Score: 0.00000, ROC_AUC: 0.52419\n",
      "Validation Results- Epoch: 7 Precision: 0.00000, Recall: 0.00000\n",
      "Validation Results- Epoch: 7 Confusion Matrix: \n",
      "[[3363    0]\n",
      " [2001    0]]\n",
      "Validation Results  - Epoch: 7 Pred Positive Stat: 0.00000, -1.00000\n",
      "Training Results  - Epoch: 8 Avg accuracy: 0.60297, Avg BCE: 0.67535, F1 Score: 0.00149, ROC_AUC: 0.53292\n",
      "Training Results  - Epoch: 8 Precision: 0.75000, Recall: 0.00075\n",
      "Training Results  - Epoch: 8 Confusion Matrix \n",
      "[[12174     2]\n",
      " [ 8018     6]]\n",
      "Training Results  - Epoch: 8 Pred Positive Stat: 0.00131, 0.00150\n",
      "Validation Results- Epoch: 8 Avg accuracy: 0.62752, Avg BCE: 0.66989, F1 Score: 0.00299, ROC_AUC: 0.53031\n",
      "Validation Results- Epoch: 8 Precision: 1.00000, Recall: 0.00150\n",
      "Validation Results- Epoch: 8 Confusion Matrix: \n",
      "[[3363    0]\n",
      " [1998    3]]\n",
      "Validation Results  - Epoch: 8 Pred Positive Stat: 0.00347, 0.00140\n",
      "Training Results  - Epoch: 9 Avg accuracy: 0.60337, Avg BCE: 0.67577, F1 Score: 0.00694, ROC_AUC: 0.54213\n",
      "Training Results  - Epoch: 9 Precision: 0.63636, Recall: 0.00349\n",
      "Training Results  - Epoch: 9 Confusion Matrix \n",
      "[[12160    16]\n",
      " [ 7996    28]]\n",
      "Training Results  - Epoch: 9 Pred Positive Stat: 0.00123, 0.00236\n",
      "Validation Results- Epoch: 9 Avg accuracy: 0.62509, Avg BCE: 0.67137, F1 Score: 0.00495, ROC_AUC: 0.53049\n",
      "Validation Results- Epoch: 9 Precision: 0.25000, Recall: 0.00250\n",
      "Validation Results- Epoch: 9 Confusion Matrix: \n",
      "[[3348   15]\n",
      " [1996    5]]\n",
      "Validation Results  - Epoch: 9 Pred Positive Stat: -0.00072, 0.00250\n",
      "Training Results  - Epoch: 10 Avg accuracy: 0.60302, Avg BCE: 0.67571, F1 Score: 0.00174, ROC_AUC: 0.52762\n",
      "Training Results  - Epoch: 10 Precision: 0.77778, Recall: 0.00087\n",
      "Training Results  - Epoch: 10 Confusion Matrix \n",
      "[[12174     2]\n",
      " [ 8017     7]]\n",
      "Training Results  - Epoch: 10 Pred Positive Stat: 0.00192, 0.00152\n",
      "Validation Results- Epoch: 10 Avg accuracy: 0.62696, Avg BCE: 0.67068, F1 Score: 0.00200, ROC_AUC: 0.52388\n",
      "Validation Results- Epoch: 10 Precision: 0.50000, Recall: 0.00100\n",
      "Validation Results- Epoch: 10 Confusion Matrix: \n",
      "[[3361    2]\n",
      " [1999    2]]\n",
      "Validation Results  - Epoch: 10 Pred Positive Stat: 0.00113, 0.00425\n",
      "Training Results  - Epoch: 11 Avg accuracy: 0.60366, Avg BCE: 0.67482, F1 Score: 0.00842, ROC_AUC: 0.53308\n",
      "Training Results  - Epoch: 11 Precision: 0.68000, Recall: 0.00424\n",
      "Training Results  - Epoch: 11 Confusion Matrix \n",
      "[[12160    16]\n",
      " [ 7990    34]]\n",
      "Training Results  - Epoch: 11 Pred Positive Stat: 0.00130, 0.00296\n",
      "Validation Results- Epoch: 11 Avg accuracy: 0.62547, Avg BCE: 0.66995, F1 Score: 0.00495, ROC_AUC: 0.52919\n",
      "Validation Results- Epoch: 11 Precision: 0.27778, Recall: 0.00250\n",
      "Validation Results- Epoch: 11 Confusion Matrix: \n",
      "[[3350   13]\n",
      " [1996    5]]\n",
      "Validation Results  - Epoch: 11 Pred Positive Stat: -0.00074, 0.00276\n",
      "Training Results  - Epoch: 12 Avg accuracy: 0.60376, Avg BCE: 0.67544, F1 Score: 0.01063, ROC_AUC: 0.53411\n",
      "Training Results  - Epoch: 12 Precision: 0.65152, Recall: 0.00536\n",
      "Training Results  - Epoch: 12 Confusion Matrix \n",
      "[[12153    23]\n",
      " [ 7981    43]]\n",
      "Training Results  - Epoch: 12 Pred Positive Stat: 0.00108, 0.00263\n",
      "Validation Results- Epoch: 12 Avg accuracy: 0.62472, Avg BCE: 0.67126, F1 Score: 0.00297, ROC_AUC: 0.51775\n",
      "Validation Results- Epoch: 12 Precision: 0.16667, Recall: 0.00150\n",
      "Validation Results- Epoch: 12 Confusion Matrix: \n",
      "[[3348   15]\n",
      " [1998    3]]\n",
      "Validation Results  - Epoch: 12 Pred Positive Stat: -0.00111, 0.00280\n",
      "Training Results  - Epoch: 13 Avg accuracy: 0.60366, Avg BCE: 0.67505, F1 Score: 0.00621, ROC_AUC: 0.54647\n",
      "Training Results  - Epoch: 13 Precision: 0.78125, Recall: 0.00312\n",
      "Training Results  - Epoch: 13 Confusion Matrix \n",
      "[[12169     7]\n",
      " [ 7999    25]]\n",
      "Training Results  - Epoch: 13 Pred Positive Stat: 0.00191, 0.00188\n",
      "Validation Results- Epoch: 13 Avg accuracy: 0.62603, Avg BCE: 0.67021, F1 Score: 0.00496, ROC_AUC: 0.54422\n",
      "Validation Results- Epoch: 13 Precision: 0.33333, Recall: 0.00250\n",
      "Validation Results- Epoch: 13 Confusion Matrix: \n",
      "[[3353   10]\n",
      " [1996    5]]\n",
      "Validation Results  - Epoch: 13 Pred Positive Stat: -0.00053, 0.00308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results  - Epoch: 14 Avg accuracy: 0.60411, Avg BCE: 0.67550, F1 Score: 0.01064, ROC_AUC: 0.53941\n",
      "Training Results  - Epoch: 14 Precision: 0.72881, Recall: 0.00536\n",
      "Training Results  - Epoch: 14 Confusion Matrix \n",
      "[[12160    16]\n",
      " [ 7981    43]]\n",
      "Training Results  - Epoch: 14 Pred Positive Stat: 0.00136, 0.00251\n",
      "Validation Results- Epoch: 14 Avg accuracy: 0.62472, Avg BCE: 0.67150, F1 Score: 0.00494, ROC_AUC: 0.52727\n",
      "Validation Results- Epoch: 14 Precision: 0.22727, Recall: 0.00250\n",
      "Validation Results- Epoch: 14 Confusion Matrix: \n",
      "[[3346   17]\n",
      " [1996    5]]\n",
      "Validation Results  - Epoch: 14 Pred Positive Stat: -0.00115, 0.00278\n",
      "Training Results  - Epoch: 15 Avg accuracy: 0.60480, Avg BCE: 0.67499, F1 Score: 0.01965, ROC_AUC: 0.54796\n",
      "Training Results  - Epoch: 15 Precision: 0.67227, Recall: 0.00997\n",
      "Training Results  - Epoch: 15 Confusion Matrix \n",
      "[[12137    39]\n",
      " [ 7944    80]]\n",
      "Training Results  - Epoch: 15 Pred Positive Stat: 0.00102, 0.00283\n",
      "Validation Results- Epoch: 15 Avg accuracy: 0.62453, Avg BCE: 0.67087, F1 Score: 0.01468, ROC_AUC: 0.54535\n",
      "Validation Results- Epoch: 15 Precision: 0.34884, Recall: 0.00750\n",
      "Validation Results- Epoch: 15 Confusion Matrix: \n",
      "[[3335   28]\n",
      " [1986   15]]\n",
      "Validation Results  - Epoch: 15 Pred Positive Stat: -0.00058, 0.00248\n",
      "Training Results  - Epoch: 16 Avg accuracy: 0.60520, Avg BCE: 0.67268, F1 Score: 0.01846, ROC_AUC: 0.54830\n",
      "Training Results  - Epoch: 16 Precision: 0.74257, Recall: 0.00935\n",
      "Training Results  - Epoch: 16 Confusion Matrix \n",
      "[[12150    26]\n",
      " [ 7949    75]]\n",
      "Training Results  - Epoch: 16 Pred Positive Stat: 0.00127, 0.00266\n",
      "Validation Results- Epoch: 16 Avg accuracy: 0.62435, Avg BCE: 0.66692, F1 Score: 0.01371, ROC_AUC: 0.56279\n",
      "Validation Results- Epoch: 16 Precision: 0.33333, Recall: 0.00700\n",
      "Validation Results- Epoch: 16 Confusion Matrix: \n",
      "[[3335   28]\n",
      " [1987   14]]\n",
      "Validation Results  - Epoch: 16 Pred Positive Stat: -0.00065, 0.00245\n",
      "Training Results  - Epoch: 17 Avg accuracy: 0.60490, Avg BCE: 0.67286, F1 Score: 0.01408, ROC_AUC: 0.54863\n",
      "Training Results  - Epoch: 17 Precision: 0.80282, Recall: 0.00710\n",
      "Training Results  - Epoch: 17 Confusion Matrix \n",
      "[[12162    14]\n",
      " [ 7967    57]]\n",
      "Training Results  - Epoch: 17 Pred Positive Stat: 0.00179, 0.00219\n",
      "Validation Results- Epoch: 17 Avg accuracy: 0.62435, Avg BCE: 0.66666, F1 Score: 0.00788, ROC_AUC: 0.55942\n",
      "Validation Results- Epoch: 17 Precision: 0.26667, Recall: 0.00400\n",
      "Validation Results- Epoch: 17 Confusion Matrix: \n",
      "[[3341   22]\n",
      " [1993    8]]\n",
      "Validation Results  - Epoch: 17 Pred Positive Stat: -0.00088, 0.00261\n",
      "Training Results  - Epoch: 18 Avg accuracy: 0.60376, Avg BCE: 0.67255, F1 Score: 0.00596, ROC_AUC: 0.53858\n",
      "Training Results  - Epoch: 18 Precision: 0.85714, Recall: 0.00299\n",
      "Training Results  - Epoch: 18 Confusion Matrix \n",
      "[[12172     4]\n",
      " [ 8000    24]]\n",
      "Training Results  - Epoch: 18 Pred Positive Stat: 0.00212, 0.00227\n",
      "Validation Results- Epoch: 18 Avg accuracy: 0.62621, Avg BCE: 0.66461, F1 Score: 0.00298, ROC_AUC: 0.56073\n",
      "Validation Results- Epoch: 18 Precision: 0.30000, Recall: 0.00150\n",
      "Validation Results- Epoch: 18 Confusion Matrix: \n",
      "[[3356    7]\n",
      " [1998    3]]\n",
      "Validation Results  - Epoch: 18 Pred Positive Stat: -0.00091, 0.00302\n",
      "Training Results  - Epoch: 19 Avg accuracy: 0.60475, Avg BCE: 0.67455, F1 Score: 0.01408, ROC_AUC: 0.55407\n",
      "Training Results  - Epoch: 19 Precision: 0.77027, Recall: 0.00710\n",
      "Training Results  - Epoch: 19 Confusion Matrix \n",
      "[[12159    17]\n",
      " [ 7967    57]]\n",
      "Training Results  - Epoch: 19 Pred Positive Stat: 0.00148, 0.00266\n",
      "Validation Results- Epoch: 19 Avg accuracy: 0.62509, Avg BCE: 0.66818, F1 Score: 0.00887, ROC_AUC: 0.57835\n",
      "Validation Results- Epoch: 19 Precision: 0.32143, Recall: 0.00450\n",
      "Validation Results- Epoch: 19 Confusion Matrix: \n",
      "[[3344   19]\n",
      " [1992    9]]\n",
      "Validation Results  - Epoch: 19 Pred Positive Stat: -0.00061, 0.00260\n",
      "Training Results  - Epoch: 20 Avg accuracy: 0.60530, Avg BCE: 0.67577, F1 Score: 0.01822, ROC_AUC: 0.55352\n",
      "Training Results  - Epoch: 20 Precision: 0.76289, Recall: 0.00922\n",
      "Training Results  - Epoch: 20 Confusion Matrix \n",
      "[[12153    23]\n",
      " [ 7950    74]]\n",
      "Training Results  - Epoch: 20 Pred Positive Stat: 0.00140, 0.00250\n",
      "Validation Results- Epoch: 20 Avg accuracy: 0.62435, Avg BCE: 0.67099, F1 Score: 0.01371, ROC_AUC: 0.57155\n",
      "Validation Results- Epoch: 20 Precision: 0.33333, Recall: 0.00700\n",
      "Validation Results- Epoch: 20 Confusion Matrix: \n",
      "[[3335   28]\n",
      " [1987   14]]\n",
      "Validation Results  - Epoch: 20 Pred Positive Stat: -0.00061, 0.00241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f08d7836c88>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_dl, max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.state.iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clone_tf]",
   "language": "python",
   "name": "conda-env-clone_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
